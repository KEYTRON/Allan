#!/usr/bin/env python3
"""
Allan Dataset Downloader - –ó–∞–≥—Ä—É–∑—á–∏–∫ –∏ –ø—Ä–µ–¥–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–∞ Google Drive
"""

import os
import sys
import time
import json
import shutil
import zipfile
import tarfile
import requests
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from dataclasses import dataclass
import psutil
import subprocess
from tqdm import tqdm
import hashlib

# –ò–º–ø–æ—Ä—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Drive –≤ Google Colab
try:
    from google.colab import drive
    IS_COLAB = True
except ImportError:
    IS_COLAB = False
    print("‚ö†Ô∏è  Google Colab –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∫–æ–¥ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ Colab.")

@dataclass
class DatasetConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏"""
    name: str
    source_url: str
    source_type: str  # 'url', 'huggingface', 'kaggle', 'local'
    format: str  # 'zip', 'tar', 'csv', 'json', 'hf_dataset'
    size_mb: float
    description: str
    language: str = "ru"
    task_type: str = "general"
    preprocessing_steps: List[str] = None
    validation_checks: List[str] = None
    dependencies: List[str] = None

class AllanDatasetDownloader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –∏ –ø—Ä–µ–¥–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Allan"""
    
    def __init__(self, project_path: Optional[str] = None):
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive –≤ Colab
        if IS_COLAB:
            self._mount_google_drive()
        
        # –í—ã–±–æ—Ä –ø—É—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏—è
        if project_path is None:
            project_path = "/content/drive/MyDrive/ML_Projects/Allan_Model" if IS_COLAB else "/workspace/Allan_Model"
        
        self.project_path = project_path
        self.datasets_path = f"{project_path}/datasets"
        self.raw_path = f"{self.datasets_path}/raw"
        self.processed_path = f"{self.datasets_path}/processed"
        self.cached_path = f"{self.datasets_path}/cached"
        self.temp_path = f"{self.datasets_path}/temp"
        self.local_cache = "/content/allan_cache" if IS_COLAB else f"{self.project_path}/.allan_cache"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        self._create_directories()
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        self.dataset_configs = self._load_dataset_configs()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
        self.chunk_size = 8192  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        self.max_retries = 3    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        self.timeout = 300      # –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

    def _mount_google_drive(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive –≤ Google Colab"""
        try:
            print("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω –ª–∏ —É–∂–µ –¥–∏—Å–∫
            if os.path.exists('/content/drive/MyDrive'):
                print("‚úÖ Google Drive —É–∂–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")
                return True
            
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º Google Drive
            drive.mount('/content/drive', force_remount=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            if os.path.exists('/content/drive/MyDrive'):
                print("‚úÖ Google Drive —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω")
                return True
            else:
                print("‚ùå –û—à–∏–±–∫–∞: Google Drive –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")
                return False
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è Google Drive: {e}")
            print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –≤—Ä—É—á–Ω—É—é:")
            print("   from google.colab import drive")
            print("   drive.mount('/content/drive')")
            return False
        
    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        directories = [
            self.project_path,
            self.datasets_path,
            self.raw_path,
            self.processed_path,
            self.cached_path,
            self.temp_path,
            self.local_cache
        ]
        
        print("üìÅ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
        for directory in directories:
            try:
                os.makedirs(directory, exist_ok=True)
                print(f"  ‚úÖ {directory}")
            except PermissionError:
                print(f"  ‚ùå –ù–µ—Ç –ø—Ä–∞–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è: {directory}")
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è {directory}: {e}")
    
    def _load_dataset_configs(self) -> Dict[str, DatasetConfig]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        configs = {
            # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
            "sberquad": DatasetConfig(
                name="sberquad",
                source_url="sberbank-ai/sberquad",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=150,
                description="–†—É—Å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ SQuAD",
                task_type="qa",
                preprocessing_steps=["tokenization", "max_length_512", "truncation"],
                validation_checks=["check_qa_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),
            
            "rucola": DatasetConfig(
                name="rucola",
                source_url="RussianNLP/rucola",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=50,
                description="–ö–æ—Ä–ø—É—Å –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∏–µ–º–ª–µ–º–æ—Å—Ç–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ",
                task_type="classification",
                preprocessing_steps=["tokenization", "max_length_128", "binary_labels"],
                validation_checks=["check_classification_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),
            
            "russian_superglue": DatasetConfig(
                name="russian_superglue",
                source_url="RussianNLP/russian_super_glue",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=200,
                description="–ù–∞–±–æ—Ä –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                task_type="multi_task",
                preprocessing_steps=["task_specific_preprocessing", "unified_format"],
                validation_checks=["check_multitask_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            # –ù–æ–≤—ã–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
            "russian_tape": DatasetConfig(
                name="russian_tape",
                source_url="RussianNLP/tape",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=300,
                description="TAPE - –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞",
                task_type="multi_task",
                preprocessing_steps=["task_specific_preprocessing", "unified_format"],
                validation_checks=["check_multitask_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "ru_paradetox": DatasetConfig(
                name="ru_paradetox",
                source_url="s-nlp/ru_paradetox",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=100,
                description="–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ—Ç–æ–∫—Å–∏–∫–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞",
                task_type="text_detoxification",
                preprocessing_steps=["text_cleaning", "create_pairs", "max_length_256"],
                validation_checks=["check_paraphrase_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "lenta_news": DatasetConfig(
                name="lenta_news",
                source_url="IlyaGusev/gazeta",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=2000,
                description="–ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ Lenta.ru",
                task_type="text_generation",
                preprocessing_steps=["text_cleaning", "max_length_1024", "remove_html"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "russian_poems": DatasetConfig(
                name="russian_poems",
                source_url="IlyaGusev/russian_poems",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=150,
                description="–ö–æ—Ä–ø—É—Å —Ä—É—Å—Å–∫–æ–π –ø–æ—ç–∑–∏–∏",
                task_type="text_generation",
                preprocessing_steps=["text_cleaning", "max_length_512", "poem_formatting"],
                validation_checks=["check_poem_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "rucos": DatasetConfig(
                name="rucos", 
                source_url="IlyaGusev/rucos",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=80,
                description="–ß—Ç–µ–Ω–∏–µ —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∏ –∑–¥—Ä–∞–≤—ã–º —Å–º—ã—Å–ª–æ–º –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞",
                task_type="reading_comprehension",
                preprocessing_steps=["tokenization", "max_length_512", "context_question_pairs"],
                validation_checks=["check_qa_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "russian_sentiment_twitter": DatasetConfig(
                name="russian_sentiment_twitter",
                source_url="Tatyana/russian_sentiment_twitter",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=200,
                description="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä—É—Å—Å–∫–∏—Ö —Ç–≤–∏—Ç–æ–≤",
                task_type="sentiment_analysis",
                preprocessing_steps=["text_cleaning", "max_length_256", "sentiment_labels"],
                validation_checks=["check_sentiment_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "russian_detox": DatasetConfig(
                name="russian_detox",
                source_url="unitary/toxic-bert",
                source_type="huggingface", 
                format="hf_dataset",
                size_mb=150,
                description="–î–µ—Ç–µ–∫—Ü–∏—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –≤ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ",
                task_type="toxicity_detection",
                preprocessing_steps=["text_cleaning", "max_length_256", "binary_labels"],
                validation_checks=["check_classification_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "russian_ner": DatasetConfig(
                name="russian_ner",
                source_url="wietsedv/wikiner_russian",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=50,
                description="–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞",
                task_type="ner",
                preprocessing_steps=["tokenization", "ner_labels", "max_length_256"],
                validation_checks=["check_ner_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "openvqa_ru": DatasetConfig(
                name="openvqa_ru",
                source_url="open-vqa/openvqa_ru",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=500,
                description="–í–∏–∑—É–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã-–æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
                task_type="visual_qa",
                preprocessing_steps=["image_text_pairs", "max_length_256", "visual_features"],
                validation_checks=["check_vqa_format", "validate_russian_text"],
                dependencies=["transformers", "datasets", "Pillow"]
            ),

            "ruwikititle": DatasetConfig(
                name="ruwikititle",
                source_url="IlyaGusev/ruwikititle",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=300,
                description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–∑ —Ä—É—Å—Å–∫–æ–π –í–∏–∫–∏–ø–µ–¥–∏–∏",
                task_type="title_generation",
                preprocessing_steps=["text_cleaning", "create_pairs", "max_length_512"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "taiga_news": DatasetConfig(
                name="taiga_news",
                source_url="https://github.com/TatianaShavrina/taiga_site/releases/download/v1.0/news.tar.gz",
                source_type="url",
                format="tar",
                size_mb=3000,
                description="–ù–æ–≤–æ—Å—Ç–Ω–æ–π –∫–æ—Ä–ø—É—Å Taiga - –æ–≥—Ä–æ–º–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è —Ä—É—Å—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π",
                task_type="text_generation",
                preprocessing_steps=["text_cleaning", "max_length_1024", "remove_html"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["pandas", "numpy"]
            ),

            "opencorpora": DatasetConfig(
                name="opencorpora",
                source_url="https://github.com/OpenCorpora/opencorpora/archive/master.zip",
                source_type="url",
                format="zip",
                size_mb=500,
                description="–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞",
                task_type="morphology",
                preprocessing_steps=["morphology_parsing", "tokenization", "pos_tagging"],
                validation_checks=["check_morphology_format", "validate_russian_text"],
                dependencies=["pandas", "numpy", "pymorphy2"]
            ),

            "rureviews": DatasetConfig(
                name="rureviews",
                source_url="sismetanin/rureviews",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=800,
                description="–û—Ç–∑—ã–≤—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ —Å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º",
                task_type="sentiment_analysis",
                preprocessing_steps=["text_cleaning", "max_length_512", "sentiment_labels"],
                validation_checks=["check_sentiment_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "russian_dialogues": DatasetConfig(
                name="russian_dialogues",
                source_url="Grossmend/oasst_ru",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=200,
                description="–î–∏–∞–ª–æ–≥–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —á–∞—Ç-–±–æ—Ç–æ–≤",
                task_type="dialogue",
                preprocessing_steps=["dialogue_formatting", "max_length_512", "conversation_pairs"],
                validation_checks=["check_dialogue_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "ru_pikabu": DatasetConfig(
                name="ru_pikabu",
                source_url="IlyaGusev/pikabu",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=1500,
                description="–ü–æ—Å—Ç—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Å —Å–∞–π—Ç–∞ Pikabu",
                task_type="text_generation",
                preprocessing_steps=["text_cleaning", "max_length_1024", "social_media_formatting"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "rutax": DatasetConfig(
                name="rutax",
                source_url="https://github.com/rutar-anonymous/RuTaR/archive/main.zip",
                source_type="url",
                format="zip",
                size_mb=50,
                description="–†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –æ –Ω–∞–ª–æ–≥–∞—Ö –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
                task_type="reasoning",
                preprocessing_steps=["text_cleaning", "max_length_512", "legal_formatting"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["pandas", "numpy"]
            ),

            "russian_literature": DatasetConfig(
                name="russian_literature",
                source_url="IlyaGusev/russian_literature",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=2000,
                description="–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è —Ä—É—Å—Å–∫–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
                task_type="text_generation",
                preprocessing_steps=["text_cleaning", "max_length_1024", "literature_formatting"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "russian_jokes": DatasetConfig(
                name="russian_jokes",
                source_url="cointegrated/russian-jokes-dataset",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=100,
                description="–†—É—Å—Å–∫–∏–µ –∞–Ω–µ–∫–¥–æ—Ç—ã –∏ —à—É—Ç–∫–∏",
                task_type="text_generation",
                preprocessing_steps=["text_cleaning", "max_length_256", "joke_formatting"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "russian_medical": DatasetConfig(
                name="russian_medical",
                source_url="d0rj/russian-medical-qa",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=300,
                description="–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
                task_type="medical_qa",
                preprocessing_steps=["text_cleaning", "max_length_512", "medical_formatting"],
                validation_checks=["check_qa_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            "russian_headlines": DatasetConfig(
                name="russian_headlines",
                source_url="IlyaGusev/headline_cause",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=250,
                description="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π",
                task_type="headline_generation",
                preprocessing_steps=["text_cleaning", "create_pairs", "max_length_256"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ —Ä–∞–Ω–µ–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ URL-–¥–∞—Ç–∞—Å–µ—Ç—ã
            "russian_paraphrase": DatasetConfig(
                name="russian_paraphrase",
                source_url="https://storage.googleapis.com/russian-paraphrase/russian_paraphrase.zip",
                source_type="url",
                format="zip",
                size_mb=80,
                description="–î–∞—Ç–∞—Å–µ—Ç —Ä—É—Å—Å–∫–∏—Ö –ø–∞—Ä–∞—Ñ—Ä–∞–∑",
                task_type="paraphrase",
                preprocessing_steps=["extract_paraphrases", "create_pairs", "max_length_256"],
                validation_checks=["check_paraphrase_format", "validate_russian_text"],
                dependencies=["pandas", "numpy"]
            ),
            "russian_sentiment": DatasetConfig(
                name="russian_sentiment",
                source_url="https://github.com/DeepPavlov/russian_sentiment/archive/refs/heads/master.zip",
                source_type="url",
                format="zip",
                size_mb=120,
                description="–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞",
                task_type="sentiment_analysis",
                preprocessing_steps=["extract_sentences", "create_labels", "max_length_256"],
                validation_checks=["check_sentiment_format", "validate_russian_text"],
                dependencies=["pandas", "numpy"]
            )
        }
        return configs
    
    def download_file_with_progress(self, url: str, destination: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞: {url}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            response = requests.head(url, timeout=10)
            total_size = int(response.headers.get('content-length', 0))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
            response = requests.get(url, stream=True, timeout=self.timeout)
            response.raise_for_status()
            
            with open(destination, 'wb') as file, tqdm(
                desc=os.path.basename(destination),
                total=total_size,
                unit='iB',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=self.chunk_size):
                    size = file.write(chunk)
                    pbar.update(size)
            
            print(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {destination}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return False
    
    def download_huggingface_dataset(self, dataset_name: str, config: DatasetConfig) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Hugging Face Hub"""
        try:
            print(f"ü§ó –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Hugging Face: {dataset_name}")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if config.dependencies:
                self._install_dependencies(config.dependencies)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            from datasets import load_dataset
            
            dataset = load_dataset(config.source_url)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ Google Drive
            save_path = f"{self.cached_path}/{dataset_name}"
            dataset.save_to_disk(save_path)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "name": dataset_name,
                "source": config.source_url,
                "source_type": config.source_type,
                "download_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "size_mb": config.size_mb,
                "description": config.description,
                "task_type": config.task_type,
                "language": config.language,
                "format": config.format,
                "preprocessing_steps": config.preprocessing_steps,
                "validation_checks": config.validation_checks
            }
            
            with open(f"{save_path}/metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return False
    
    def download_url_dataset(self, dataset_name: str, config: DatasetConfig) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ URL"""
        try:
            print(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ URL: {dataset_name}")
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
            temp_dir = f"{self.temp_path}/{dataset_name}"
            os.makedirs(temp_dir, exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            filename = os.path.basename(config.source_url)
            if not filename or '.' not in filename:
                filename = f"{dataset_name}.zip"
            
            temp_file = f"{temp_dir}/{filename}"
            
            if not self.download_file_with_progress(config.source_url, temp_file):
                return False
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ä—Ö–∏–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if config.format in ['zip', 'tar', 'tar.gz']:
                extract_dir = f"{temp_dir}/extracted"
                os.makedirs(extract_dir, exist_ok=True)
                
                if not self._extract_archive(temp_file, extract_dir):
                    return False
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                raw_dir = f"{self.raw_path}/{dataset_name}"
                if os.path.exists(raw_dir):
                    shutil.rmtree(raw_dir)
                shutil.move(extract_dir, raw_dir)
                
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                shutil.rmtree(temp_dir)
                
                print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω: {raw_dir}")
            else:
                # –ü—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª
                raw_dir = f"{self.raw_path}/{dataset_name}"
                os.makedirs(raw_dir, exist_ok=True)
                shutil.copy2(temp_file, f"{raw_dir}/{filename}")
                shutil.rmtree(temp_dir)
                
                print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {raw_dir}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ URL –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return False
    
    def _extract_archive(self, archive_path: str, extract_to: str) -> bool:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞"""
        try:
            print(f"üì¶ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞: {os.path.basename(archive_path)}")
            
            if archive_path.endswith('.zip'):
                with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_to)
            elif archive_path.endswith(('.tar', '.tar.gz', '.tgz')):
                with tarfile.open(archive_path, 'r:*') as tar_ref:
                    tar_ref.extractall(extract_to)
            else:
                print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∞—Ä—Ö–∏–≤–∞: {archive_path}")
                return False
            
            print("‚úÖ –ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—Ä—Ö–∏–≤–∞: {e}")
            return False
    
    def _install_dependencies(self, dependencies: List[str]):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
        print(f"üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {', '.join(dependencies)}")
        
        for dep in dependencies:
            try:
                subprocess.run([
                    sys.executable, "-m", "pip", "install", "-q", dep
                ], check=True, capture_output=True)
                print(f"  ‚úÖ {dep} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            except subprocess.CalledProcessError as e:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {dep}: {e}")

    def check_google_drive_space(self) -> Dict[str, float]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ Google Drive"""
        try:
            total, used, free = shutil.disk_usage("/content/drive/MyDrive")
            return {
                "total_gb": total / (1024**3),
                "used_gb": used / (1024**3),
                "free_gb": free / (1024**3),
                "free_percent": (free / total) * 100
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–µ—Å—Ç–∞: {e}")
            return {}
    
    def preprocess_dataset(self, dataset_name: str, config: DatasetConfig) -> bool:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            print(f"üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
            
            raw_path = f"{self.raw_path}/{dataset_name}"
            processed_path = f"{self.processed_path}/{dataset_name}"
            
            if not os.path.exists(raw_path):
                print(f"‚ùå –°—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {raw_path}")
                return False
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            os.makedirs(processed_path, exist_ok=True)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–≥–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            if config.preprocessing_steps:
                for step in config.preprocessing_steps:
                    print(f"  üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: {step}")
                    
                    if step == "text_cleaning":
                        self._clean_text_data(raw_path, processed_path)
                    elif step == "tokenization":
                        self._tokenize_data(raw_path, processed_path)
                    elif step == "max_length_512":
                        self._truncate_data(processed_path, 512)
                    elif step == "max_length_256":
                        self._truncate_data(processed_path, 256)
                    elif step == "max_length_128":
                        self._truncate_data(processed_path, 128)
                    elif step == "max_length_1024":
                        self._truncate_data(processed_path, 1024)
                    elif step == "remove_html":
                        self._remove_html_tags(processed_path)
                    elif step == "poem_formatting":
                        self._format_poems(processed_path)
                    elif step == "binary_labels":
                        self._convert_to_binary_labels(processed_path)
                    elif step == "create_pairs":
                        self._create_paraphrase_pairs(processed_path)
                    elif step == "extract_paraphrases":
                        self._extract_paraphrases(raw_path, processed_path)
                    elif step == "extract_sentences":
                        self._extract_sentences(raw_path, processed_path)
                    elif step == "create_labels":
                        self._create_sentiment_labels(processed_path)
                    elif step == "task_specific_preprocessing":
                        self._task_specific_preprocessing(raw_path, processed_path)
                    elif step == "unified_format":
                        self._unify_format(processed_path)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            preprocessing_metadata = {
                "dataset_name": dataset_name,
                "preprocessing_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "applied_steps": config.preprocessing_steps or [],
                "original_config": {
                    "size_mb": config.size_mb,
                    "description": config.description,
                    "task_type": config.task_type,
                    "language": config.language
                }
            }
            
            with open(f"{processed_path}/preprocessing_metadata.json", 'w', encoding='utf-8') as f:
                json.dump(preprocessing_metadata, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {processed_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return False
    
    def _clean_text_data(self, raw_path: str, processed_path: str):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        pass
    
    def _tokenize_data(self, raw_path: str, processed_path: str):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        pass
    
    def _truncate_data(self, processed_path: str, max_length: int):
        """–û–±—Ä–µ–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã"""
        # –û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        pass
    
    def _remove_html_tags(self, processed_path: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤"""
        # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ä–∞–∑–º–µ—Ç–∫–∏
        pass
    
    def _format_poems(self, processed_path: str):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∏—Ö–æ–≤"""
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        pass
    
    def _convert_to_binary_labels(self, processed_path: str):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–µ—Ç–æ–∫ –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        pass
    
    def _create_paraphrase_pairs(self, processed_path: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä –ø–∞—Ä–∞—Ñ—Ä–∞–∑"""
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä –ø–∞—Ä–∞—Ñ—Ä–∞–∑
        pass
    
    def _extract_paraphrases(self, raw_path: str, processed_path: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞—Ñ—Ä–∞–∑"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞—Ñ—Ä–∞–∑ –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        pass
    
    def _extract_sentences(self, raw_path: str, processed_path: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞
        pass
    
    def _create_sentiment_labels(self, processed_path: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        pass
    
    def _task_specific_preprocessing(self, raw_path: str, processed_path: str):
        """–ó–∞–¥–∞—á–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏
        pass
    
    def _unify_format(self, processed_path: str):
        """–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞"""
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
        pass
    
    def validate_dataset(self, dataset_name: str, config: DatasetConfig) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            print(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
            
            processed_path = f"{self.processed_path}/{dataset_name}"
            
            if not os.path.exists(processed_path):
                print(f"‚ùå –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {processed_path}")
                return False
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if config.validation_checks:
                for check in config.validation_checks:
                    print(f"  ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: {check}")
                    
                    if check == "check_qa_format":
                        if not self._validate_qa_format(processed_path):
                            return False
                    elif check == "validate_russian_text":
                        if not self._validate_russian_text(processed_path):
                            return False
                    elif check == "check_classification_format":
                        if not self._validate_classification_format(processed_path):
                            return False
                    elif check == "check_multitask_format":
                        if not self._validate_multitask_format(processed_path):
                            return False
                    elif check == "check_text_format":
                        if not self._validate_text_format(processed_path):
                            return False
                    elif check == "check_poem_format":
                        if not self._validate_poem_format(processed_path):
                            return False
                    elif check == "check_paraphrase_format":
                        if not self._validate_paraphrase_format(processed_path):
                            return False
                    elif check == "check_sentiment_format":
                        if not self._validate_sentiment_format(processed_path):
                            return False
            
            print(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return False
    
    def _validate_qa_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ QA
        return True
    
    def _validate_russian_text(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        return True
    
    def _validate_classification_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        return True
    
    def _validate_multitask_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–æ—Å—Ç–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–æ—Å—Ç–∏
        return True
    
    def _validate_text_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        return True
    
    def _validate_poem_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Ç–∏—Ö–æ–≤"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Ç–∏—Ö–æ–≤
        return True
    
    def _validate_paraphrase_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –ø–∞—Ä–∞—Ñ—Ä–∞–∑"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –ø–∞—Ä–∞—Ñ—Ä–∞–∑
        return True
    
    def _validate_sentiment_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        return True
    
    def download_and_preprocess(self, dataset_name: str, skip_preprocessing: bool = False) -> bool:
        """–ü–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            print(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
            print("=" * 60)
            
            if dataset_name not in self.dataset_configs:
                print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {dataset_name}")
                return False
            
            config = self.dataset_configs[dataset_name]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ (–≤ Colab)
            space_info = self.check_google_drive_space()
            if space_info and space_info.get('free_gb', 0) < (config.size_mb / 1024 * 2):  # x2 –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–∞–ª–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ!")
                print(f"   –¢—Ä–µ–±—É–µ—Ç—Å—è: ~{config.size_mb/1024:.1f} –ì–ë")
                print(f"   –î–æ—Å—Ç—É–ø–Ω–æ: {space_info['free_gb']:.1f} –ì–ë")
            
            # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞
            print(f"\nüì• –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ '{dataset_name}'")
            if config.source_type == "huggingface":
                if not self.download_huggingface_dataset(dataset_name, config):
                    return False
            elif config.source_type == "url":
                if not self.download_url_dataset(dataset_name, config):
                    return False
            else:
                print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {config.source_type}")
                return False
            
            # –®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–µ—Å–ª–∏ –Ω–µ –ø—Ä–æ–ø—É—â–µ–Ω–∞)
            if not skip_preprocessing and config.preprocessing_steps:
                print(f"\nüîß –®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ '{dataset_name}'")
                if not self.preprocess_dataset(dataset_name, config):
                    return False
            
            # –®–∞–≥ 3: –í–∞–ª–∏–¥–∞—Ü–∏—è
            if config.validation_checks:
                print(f"\nüîç –®–∞–≥ 3: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ '{dataset_name}'")
                if not self.validate_dataset(dataset_name, config):
                    return False
            
            print(f"\nüéâ –î–∞—Ç–∞—Å–µ—Ç '{dataset_name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω!")
            print(f"üìÅ –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ: {self.raw_path}/{dataset_name}")
            if not skip_preprocessing:
                print(f"üîß –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {self.processed_path}/{dataset_name}")
            print(f"üíæ –ö—ç—à: {self.cached_path}/{dataset_name}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return False

    def batch_download(self, dataset_names: List[str], max_parallel: int = 2) -> Dict[str, bool]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        results = {}
        
        print(f"üì¶ –ü–∞–∫–µ—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ {len(dataset_names)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
        print(f"üîÑ –ú–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫: {max_parallel}")
        
        for i, dataset_name in enumerate(dataset_names, 1):
            print(f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(dataset_names)} - {dataset_name}")
            results[dataset_name] = self.download_and_preprocess(dataset_name)
            
            if i % max_parallel == 0:
                print("‚è∏Ô∏è  –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–≥—Ä—É–∑–∫–∞–º–∏...")
                time.sleep(5)
        
        # –û—Ç—á–µ—Ç –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        successful = sum(1 for success in results.values() if success)
        print(f"\nüìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–∫–µ—Ç–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏:")
        print(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ: {successful}/{len(dataset_names)}")
        print(f"  ‚ùå –û—à–∏–±–∫–∏: {len(dataset_names) - successful}/{len(dataset_names)}")
        
        return results
    
    def list_available_datasets(self):
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏"""
        print("üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏:")
        print("=" * 80)
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á
        by_task: Dict[str, List[Tuple[str, DatasetConfig]]] = {}
        for name, config in self.dataset_configs.items():
            task = config.task_type
            if task not in by_task:
                by_task[task] = []
            by_task[task].append((name, config))
        
        for task_type, datasets in by_task.items():
            print(f"\nüéØ **{task_type.upper()}**")
            print("-" * 50)
            
            for name, config in datasets:
                print(f"\nüîπ **{name}**")
                print(f"  üìä –†–∞–∑–º–µ—Ä: {config.size_mb} –ú–ë")
                print(f"  üåê –ò—Å—Ç–æ—á–Ω–∏–∫: {config.source_type}")
                print(f"  üìù {config.description}")
                print(f"  üîó URL: {config.source_url}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_size = sum(config.size_mb for config in self.dataset_configs.values())
        print(f"\nüìä **–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:**")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {len(self.dataset_configs)}")
        print(f"  ‚Ä¢ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:.1f} –ú–ë ({total_size/1024:.1f} –ì–ë)")
        print(f"  ‚Ä¢ –¢–∏–ø–æ–≤ –∑–∞–¥–∞—á: {len(set(config.task_type for config in self.dataset_configs.values()))}")

    def suggest_datasets_by_task(self, task_type: str) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏"""
        suggestions: List[str] = []
        for name, config in self.dataset_configs.items():
            if config.task_type == task_type:
                suggestions.append(name)
        
        if suggestions:
            print(f"üìã –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–¥–∞—á–∏ '{task_type}':")
            for dataset in suggestions:
                config = self.dataset_configs[dataset]
                print(f"  ‚Ä¢ {dataset} ({config.size_mb} –ú–ë) - {config.description}")
        else:
            print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–¥–∞—á–∏ '{task_type}' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        return suggestions

    def recommend_datasets_by_size(self, max_size_gb: float = 2.0) -> List[str]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –ø–æ —Ä–∞–∑–º–µ—Ä—É"""
        max_size_mb = max_size_gb * 1024
        recommendations: List[Tuple[str, DatasetConfig]] = []
        
        for name, config in self.dataset_configs.items():
            if config.size_mb <= max_size_mb:
                recommendations.append((name, config))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É
        recommendations.sort(key=lambda x: x[1].size_mb, reverse=True)
        
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç—ã —Ä–∞–∑–º–µ—Ä–æ–º –¥–æ {max_size_gb} –ì–ë:")
        for name, config in recommendations:
            print(f"  ‚Ä¢ {name} ({config.size_mb} –ú–ë) - {config.task_type}")
        
        return [name for name, _ in recommendations]

    def export_dataset_list(self, output_format: str = "json") -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç —Å–ø–∏—Å–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ —Ñ–∞–π–ª"""
        try:
            export_data: Dict[str, Dict[str, Union[str, float]]] = {}
            for name, config in self.dataset_configs.items():
                export_data[name] = {
                    "source_url": config.source_url,
                    "source_type": config.source_type,
                    "format": config.format,
                    "size_mb": config.size_mb,
                    "description": config.description,
                    "language": config.language,
                    "task_type": config.task_type
                }

            timestamp = time.strftime("%Y%m%d_%H%M%S")
            
            if output_format.lower() == "json":
                filename = f"datasets_list_{timestamp}.json"
                filepath = f"{self.datasets_path}/{filename}"
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            elif output_format.lower() == "csv":
                import pandas as pd
                filename = f"datasets_list_{timestamp}.csv"
                filepath = f"{self.datasets_path}/{filename}"
                df = pd.DataFrame.from_dict(export_data, orient='index')
                df.to_csv(filepath, encoding='utf-8', index_label='dataset_name')

            print(f"üìÑ –°–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {filepath}")
            return filepath

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
            return ""
    
    def get_dataset_status(self, dataset_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        status = {
            "name": dataset_name,
            "raw_exists": False,
            "processed_exists": False,
            "cached_exists": False,
            "raw_size_mb": 0,
            "processed_size_mb": 0,
            "cached_size_mb": 0,
            "last_modified": None
        }
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            raw_path = f"{self.raw_path}/{dataset_name}"
            if os.path.exists(raw_path):
                status["raw_exists"] = True
                status["raw_size_mb"] = self._get_directory_size_mb(raw_path)
                status["last_modified"] = time.ctime(os.path.getmtime(raw_path))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            processed_path = f"{self.processed_path}/{dataset_name}"
            if os.path.exists(processed_path):
                status["processed_exists"] = True
                status["processed_size_mb"] = self._get_directory_size_mb(processed_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cached_path = f"{self.cached_path}/{dataset_name}"
            if os.path.exists(cached_path):
                status["cached_exists"] = True
                status["cached_size_mb"] = self._get_directory_size_mb(cached_path)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {e}")
        
        return status
    
    def _get_directory_size_mb(self, directory: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ú–ë"""
        try:
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(directory):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    try:
                        total_size += os.path.getsize(filepath)
                    except (OSError, IOError):
                        continue
            return total_size / (1024 * 1024)
        except:
            return 0.0
    
    def cleanup_temp_files(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            if os.path.exists(self.temp_path):
                shutil.rmtree(self.temp_path)
                os.makedirs(self.temp_path, exist_ok=True)
                print("üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –æ—á–∏—â–µ–Ω—ã")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
    
    def get_disk_usage(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –¥–∏—Å–∫–∞"""
        usage = {}
        
        try:
            # –û–±—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            total, used, free = shutil.disk_usage(self.project_path)
            usage["total_gb"] = total / (1024**3)
            usage["used_gb"] = used / (1024**3)
            usage["free_gb"] = free / (1024**3)
            usage["used_percent"] = (used / total) * 100
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ –ø–∞–ø–∫–∞–º
            usage["datasets_gb"] = self._get_directory_size_mb(self.datasets_path) / 1024
            usage["raw_gb"] = self._get_directory_size_mb(self.raw_path) / 1024
            usage["processed_gb"] = self._get_directory_size_mb(self.processed_path) / 1024
            usage["cached_gb"] = self._get_directory_size_mb(self.cached_path) / 1024
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∏—Å–∫–µ: {e}")
        
        return usage


def quick_download_dataset(dataset_name: str, skip_preprocessing: bool = False) -> bool:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    downloader = AllanDatasetDownloader()
    return downloader.download_and_preprocess(dataset_name, skip_preprocessing)


def list_downloadable_datasets():
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    downloader = AllanDatasetDownloader()
    downloader.list_available_datasets()


def get_dataset_status(dataset_name: str) -> Dict[str, Any]:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    downloader = AllanDatasetDownloader()
    return downloader.get_dataset_status(dataset_name)


def batch_download_recommended(task_type: str = None, max_size_gb: float = 2.0) -> Dict[str, bool]:
    """–ü–∞–∫–µ—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    downloader = AllanDatasetDownloader()
    
    if task_type:
        datasets = downloader.suggest_datasets_by_task(task_type)
    else:
        datasets = downloader.recommend_datasets_by_size(max_size_gb)
    
    if datasets:
        return downloader.batch_download(datasets[:5])  # –ú–∞–∫—Å–∏–º—É–º 5 –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∑–∞ —Ä–∞–∑
    else:
        print("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        return {}


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    downloader = AllanDatasetDownloader()

    print("üî• Allan Dataset Downloader - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è (–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)")
    print("=" * 70)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ
    space_info = downloader.check_google_drive_space()
    if space_info:
        print(f"\nüíæ **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Google Drive:**")
        print(f"  ‚Ä¢ –í—Å–µ–≥–æ: {space_info['total_gb']:.1f} –ì–ë")
        print(f"  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {space_info['used_gb']:.1f} –ì–ë")
        print(f"  ‚Ä¢ –°–≤–æ–±–æ–¥–Ω–æ: {space_info['free_gb']:.1f} –ì–ë ({space_info['free_percent']:.1f}%)")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    downloader.list_available_datasets()

    print(f"\nüöÄ **–ì–æ—Ç–æ–≤–æ –∫ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤!**")
    print("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: downloader.download_and_preprocess('dataset_name')")
