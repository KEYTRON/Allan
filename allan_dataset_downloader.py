#!/usr/bin/env python3
"""
Allan Dataset Downloader - –ó–∞–≥—Ä—É–∑—á–∏–∫ –∏ –ø—Ä–µ–¥–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω–∞ Google Drive
"""

import os
import sys
import time
import json
import shutil
import zipfile
import tarfile
import requests
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from dataclasses import dataclass
import psutil
import subprocess
from tqdm import tqdm
import hashlib

@dataclass
class DatasetConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏"""
    name: str
    source_url: str
    source_type: str  # 'url', 'huggingface', 'kaggle', 'local'
    format: str  # 'zip', 'tar', 'csv', 'json', 'hf_dataset'
    size_mb: float
    description: str
    language: str = "ru"
    task_type: str = "general"
    preprocessing_steps: List[str] = None
    validation_checks: List[str] = None
    dependencies: List[str] = None

class AllanDatasetDownloader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –∏ –ø—Ä–µ–¥–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Allan"""
    
    def __init__(self, project_path: str = "/content/drive/MyDrive/ML_Projects/Allan_Model"):
        self.project_path = project_path
        self.datasets_path = f"{project_path}/datasets"
        self.raw_path = f"{self.datasets_path}/raw"
        self.processed_path = f"{self.datasets_path}/processed"
        self.cached_path = f"{self.datasets_path}/cached"
        self.temp_path = f"{self.datasets_path}/temp"
        self.local_cache = "/content/allan_cache"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        self._create_directories()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        self.dataset_configs = self._load_dataset_configs()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
        self.chunk_size = 8192  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        self.max_retries = 3    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        self.timeout = 300      # –¢–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        
    def _create_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        directories = [
            self.datasets_path,
            self.raw_path,
            self.processed_path,
            self.cached_path,
            self.temp_path,
            self.local_cache
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def _load_dataset_configs(self) -> Dict[str, DatasetConfig]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        configs = {
            "sberquad": DatasetConfig(
                name="sberquad",
                source_url="sberbank-ai/sberquad",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=150,
                description="–†—É—Å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ SQuAD",
                task_type="qa",
                preprocessing_steps=["tokenization", "max_length_512", "truncation"],
                validation_checks=["check_qa_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),
            "rucola": DatasetConfig(
                name="rucola",
                source_url="RussianNLP/rucola",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=50,
                description="–ö–æ—Ä–ø—É—Å –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∏–µ–º–ª–µ–º–æ—Å—Ç–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ",
                task_type="classification",
                preprocessing_steps=["tokenization", "max_length_128", "binary_labels"],
                validation_checks=["check_classification_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),
            "russian_superglue": DatasetConfig(
                name="russian_superglue",
                source_url="russian-nlp/russian-superglue",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=200,
                description="–ù–∞–±–æ—Ä –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                task_type="multi_task",
                preprocessing_steps=["task_specific_preprocessing", "unified_format"],
                validation_checks=["check_multitask_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),
            "lenta_news": DatasetConfig(
                name="lenta_news",
                source_url="IlyaGusev/gazeta",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=2000,
                description="–ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ Lenta.ru",
                task_type="text_generation",
                preprocessing_steps=["text_cleaning", "max_length_1024", "remove_html"],
                validation_checks=["check_text_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),
            "russian_poems": DatasetConfig(
                name="russian_poems",
                source_url="IlyaGusev/russian_poems",
                source_type="huggingface",
                format="hf_dataset",
                size_mb=150,
                description="–ö–æ—Ä–ø—É—Å —Ä—É—Å—Å–∫–æ–π –ø–æ—ç–∑–∏–∏",
                task_type="text_generation",
                preprocessing_steps=["text_cleaning", "max_length_512", "poem_formatting"],
                validation_checks=["check_poem_format", "validate_russian_text"],
                dependencies=["transformers", "datasets"]
            ),
            "russian_paraphrase": DatasetConfig(
                name="russian_paraphrase",
                source_url="https://storage.googleapis.com/russian-paraphrase/russian_paraphrase.zip",
                source_type="url",
                format="zip",
                size_mb=80,
                description="–î–∞—Ç–∞—Å–µ—Ç —Ä—É—Å—Å–∫–∏—Ö –ø–∞—Ä–∞—Ñ—Ä–∞–∑",
                task_type="paraphrase",
                preprocessing_steps=["extract_paraphrases", "create_pairs", "max_length_256"],
                validation_checks=["check_paraphrase_format", "validate_russian_text"],
                dependencies=["pandas", "numpy"]
            ),
            "russian_sentiment": DatasetConfig(
                name="russian_sentiment",
                source_url="https://github.com/DeepPavlov/russian_sentiment/archive/refs/heads/master.zip",
                source_type="url",
                format="zip",
                size_mb=120,
                description="–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞",
                task_type="sentiment_analysis",
                preprocessing_steps=["extract_sentences", "create_labels", "max_length_256"],
                validation_checks=["check_sentiment_format", "validate_russian_text"],
                dependencies=["pandas", "numpy"]
            )
        }
        return configs
    
    def download_file_with_progress(self, url: str, destination: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞: {url}")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            response = requests.head(url, timeout=10)
            total_size = int(response.headers.get('content-length', 0))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
            response = requests.get(url, stream=True, timeout=self.timeout)
            response.raise_for_status()
            
            with open(destination, 'wb') as file, tqdm(
                desc=os.path.basename(destination),
                total=total_size,
                unit='iB',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=self.chunk_size):
                    size = file.write(chunk)
                    pbar.update(size)
            
            print(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {destination}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return False
    
    def download_huggingface_dataset(self, dataset_name: str, config: DatasetConfig) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Hugging Face Hub"""
        try:
            print(f"ü§ó –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Hugging Face: {dataset_name}")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if config.dependencies:
                self._install_dependencies(config.dependencies)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            from datasets import load_dataset
            
            dataset = load_dataset(config.source_url)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ Google Drive
            save_path = f"{self.cached_path}/{dataset_name}"
            dataset.save_to_disk(save_path)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "name": dataset_name,
                "source": config.source_url,
                "source_type": config.source_type,
                "download_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "size_mb": config.size_mb,
                "description": config.description,
                "task_type": config.task_type,
                "language": config.language,
                "format": config.format,
                "preprocessing_steps": config.preprocessing_steps,
                "validation_checks": config.validation_checks
            }
            
            with open(f"{save_path}/metadata.json", 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Hugging Face –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return False
    
    def download_url_dataset(self, dataset_name: str, config: DatasetConfig) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ URL"""
        try:
            print(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ URL: {dataset_name}")
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
            temp_dir = f"{self.temp_path}/{dataset_name}"
            os.makedirs(temp_dir, exist_ok=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
            filename = os.path.basename(config.source_url)
            if not filename or '.' not in filename:
                filename = f"{dataset_name}.zip"
            
            temp_file = f"{temp_dir}/{filename}"
            
            if not self.download_file_with_progress(config.source_url, temp_file):
                return False
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Ä—Ö–∏–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if config.format in ['zip', 'tar']:
                extract_dir = f"{temp_dir}/extracted"
                os.makedirs(extract_dir, exist_ok=True)
                
                if not self._extract_archive(temp_file, extract_dir):
                    return False
                
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                raw_dir = f"{self.raw_path}/{dataset_name}"
                if os.path.exists(raw_dir):
                    shutil.rmtree(raw_dir)
                shutil.move(extract_dir, raw_dir)
                
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                shutil.rmtree(temp_dir)
                
                print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω: {raw_dir}")
            else:
                # –ü—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª
                raw_dir = f"{self.raw_path}/{dataset_name}"
                os.makedirs(raw_dir, exist_ok=True)
                shutil.copy2(temp_file, f"{raw_dir}/{filename}")
                shutil.rmtree(temp_dir)
                
                print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {raw_dir}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ URL –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return False
    
    def _extract_archive(self, archive_path: str, extract_to: str) -> bool:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞"""
        try:
            print(f"üì¶ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞: {os.path.basename(archive_path)}")
            
            if archive_path.endswith('.zip'):
                with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_to)
            elif archive_path.endswith(('.tar', '.tar.gz', '.tgz')):
                with tarfile.open(archive_path, 'r:*') as tar_ref:
                    tar_ref.extractall(extract_to)
            else:
                print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∞—Ä—Ö–∏–≤–∞: {archive_path}")
                return False
            
            print("‚úÖ –ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—Ä—Ö–∏–≤–∞: {e}")
            return False
    
    def _install_dependencies(self, dependencies: List[str]):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
        print(f"üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {', '.join(dependencies)}")
        
        for dep in dependencies:
            try:
                subprocess.run([
                    sys.executable, "-m", "pip", "install", "-q", dep
                ], check=True, capture_output=True)
                print(f"  ‚úÖ {dep} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            except subprocess.CalledProcessError as e:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {dep}: {e}")
    
    def preprocess_dataset(self, dataset_name: str, config: DatasetConfig) -> bool:
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            print(f"üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
            
            raw_path = f"{self.raw_path}/{dataset_name}"
            processed_path = f"{self.processed_path}/{dataset_name}"
            
            if not os.path.exists(raw_path):
                print(f"‚ùå –°—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {raw_path}")
                return False
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            os.makedirs(processed_path, exist_ok=True)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–≥–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            if config.preprocessing_steps:
                for step in config.preprocessing_steps:
                    print(f"  üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: {step}")
                    
                    if step == "text_cleaning":
                        self._clean_text_data(raw_path, processed_path)
                    elif step == "tokenization":
                        self._tokenize_data(raw_path, processed_path)
                    elif step == "max_length_512":
                        self._truncate_data(processed_path, 512)
                    elif step == "max_length_256":
                        self._truncate_data(processed_path, 256)
                    elif step == "max_length_128":
                        self._truncate_data(processed_path, 128)
                    elif step == "max_length_1024":
                        self._truncate_data(processed_path, 1024)
                    elif step == "remove_html":
                        self._remove_html_tags(processed_path)
                    elif step == "poem_formatting":
                        self._format_poems(processed_path)
                    elif step == "binary_labels":
                        self._convert_to_binary_labels(processed_path)
                    elif step == "create_pairs":
                        self._create_paraphrase_pairs(processed_path)
                    elif step == "extract_paraphrases":
                        self._extract_paraphrases(raw_path, processed_path)
                    elif step == "extract_sentences":
                        self._extract_sentences(raw_path, processed_path)
                    elif step == "create_labels":
                        self._create_sentiment_labels(processed_path)
                    elif step == "task_specific_preprocessing":
                        self._task_specific_preprocessing(raw_path, processed_path)
                    elif step == "unified_format":
                        self._unify_format(processed_path)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            preprocessing_metadata = {
                "dataset_name": dataset_name,
                "preprocessing_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                "applied_steps": config.preprocessing_steps or [],
                "original_config": {
                    "size_mb": config.size_mb,
                    "description": config.description,
                    "task_type": config.task_type,
                    "language": config.language
                }
            }
            
            with open(f"{processed_path}/preprocessing_metadata.json", 'w', encoding='utf-8') as f:
                json.dump(preprocessing_metadata, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {processed_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return False
    
    def _clean_text_data(self, raw_path: str, processed_path: str):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        # –ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        pass
    
    def _tokenize_data(self, raw_path: str, processed_path: str):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        pass
    
    def _truncate_data(self, processed_path: str, max_length: int):
        """–û–±—Ä–µ–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã"""
        # –û–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
        pass
    
    def _remove_html_tags(self, processed_path: str):
        """–£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤"""
        # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ä–∞–∑–º–µ—Ç–∫–∏
        pass
    
    def _format_poems(self, processed_path: str):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∏—Ö–æ–≤"""
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        pass
    
    def _convert_to_binary_labels(self, processed_path: str):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –º–µ—Ç–∫–∏"""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–µ—Ç–æ–∫ –≤ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        pass
    
    def _create_paraphrase_pairs(self, processed_path: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä –ø–∞—Ä–∞—Ñ—Ä–∞–∑"""
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä –ø–∞—Ä–∞—Ñ—Ä–∞–∑
        pass
    
    def _extract_paraphrases(self, raw_path: str, processed_path: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞—Ñ—Ä–∞–∑"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞—Ñ—Ä–∞–∑ –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        pass
    
    def _extract_sentences(self, raw_path: str, processed_path: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞
        pass
    
    def _create_sentiment_labels(self, processed_path: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        pass
    
    def _task_specific_preprocessing(self, raw_path: str, processed_path: str):
        """–ó–∞–¥–∞—á–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏
        pass
    
    def _unify_format(self, processed_path: str):
        """–£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞"""
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
        pass
    
    def validate_dataset(self, dataset_name: str, config: DatasetConfig) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            print(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
            
            processed_path = f"{self.processed_path}/{dataset_name}"
            
            if not os.path.exists(processed_path):
                print(f"‚ùå –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {processed_path}")
                return False
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if config.validation_checks:
                for check in config.validation_checks:
                    print(f"  ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞: {check}")
                    
                    if check == "check_qa_format":
                        if not self._validate_qa_format(processed_path):
                            return False
                    elif check == "validate_russian_text":
                        if not self._validate_russian_text(processed_path):
                            return False
                    elif check == "check_classification_format":
                        if not self._validate_classification_format(processed_path):
                            return False
                    elif check == "check_multitask_format":
                        if not self._validate_multitask_format(processed_path):
                            return False
                    elif check == "check_text_format":
                        if not self._validate_text_format(processed_path):
                            return False
                    elif check == "check_poem_format":
                        if not self._validate_poem_format(processed_path):
                            return False
                    elif check == "check_paraphrase_format":
                        if not self._validate_paraphrase_format(processed_path):
                            return False
                    elif check == "check_sentiment_format":
                        if not self._validate_sentiment_format(processed_path):
                            return False
            
            print(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return False
    
    def _validate_qa_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ QA
        return True
    
    def _validate_russian_text(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        return True
    
    def _validate_classification_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        return True
    
    def _validate_multitask_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–æ—Å—Ç–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –º—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω–æ—Å—Ç–∏
        return True
    
    def _validate_text_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
        return True
    
    def _validate_poem_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Ç–∏—Ö–æ–≤"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Å—Ç–∏—Ö–æ–≤
        return True
    
    def _validate_paraphrase_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –ø–∞—Ä–∞—Ñ—Ä–∞–∑"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –ø–∞—Ä–∞—Ñ—Ä–∞–∑
        return True
    
    def _validate_sentiment_format(self, processed_path: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        return True
    
    def download_and_preprocess(self, dataset_name: str, skip_preprocessing: bool = False) -> bool:
        """–ü–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            print(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_name}")
            print("=" * 60)
            
            if dataset_name not in self.dataset_configs:
                print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {dataset_name}")
                return False
            
            config = self.dataset_configs[dataset_name]
            
            # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞
            print(f"\nüì• –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ '{dataset_name}'")
            if config.source_type == "huggingface":
                if not self.download_huggingface_dataset(dataset_name, config):
                    return False
            elif config.source_type == "url":
                if not self.download_url_dataset(dataset_name, config):
                    return False
            else:
                print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {config.source_type}")
                return False
            
            # –®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–µ—Å–ª–∏ –Ω–µ –ø—Ä–æ–ø—É—â–µ–Ω–∞)
            if not skip_preprocessing and config.preprocessing_steps:
                print(f"\nüîß –®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ '{dataset_name}'")
                if not self.preprocess_dataset(dataset_name, config):
                    return False
            
            # –®–∞–≥ 3: –í–∞–ª–∏–¥–∞—Ü–∏—è
            if config.validation_checks:
                print(f"\nüîç –®–∞–≥ 3: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ '{dataset_name}'")
                if not self.validate_dataset(dataset_name, config):
                    return False
            
            print(f"\nüéâ –î–∞—Ç–∞—Å–µ—Ç '{dataset_name}' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω!")
            print(f"üìÅ –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ: {self.raw_path}/{dataset_name}")
            if not skip_preprocessing:
                print(f"üîß –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {self.processed_path}/{dataset_name}")
            print(f"üíæ –ö—ç—à: {self.cached_path}/{dataset_name}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return False
    
    def list_available_datasets(self):
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏"""
        print("üìö –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏:")
        print("=" * 80)
        
        for name, config in self.dataset_configs.items():
            print(f"\nüîπ {name}")
            print(f"  üìä –†–∞–∑–º–µ—Ä: {config.size_mb} –ú–ë")
            print(f"  üéØ –ó–∞–¥–∞—á–∞: {config.task_type}")
            print(f"  üåê –ò—Å—Ç–æ—á–Ω–∏–∫: {config.source_type}")
            print(f"  üìù {config.description}")
            print(f"  üîó URL: {config.source_url}")
            
            if config.preprocessing_steps:
                print(f"  üîß –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: {', '.join(config.preprocessing_steps)}")
            
            if config.validation_checks:
                print(f"  ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è: {', '.join(config.validation_checks)}")
    
    def get_dataset_status(self, dataset_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        status = {
            "name": dataset_name,
            "raw_exists": False,
            "processed_exists": False,
            "cached_exists": False,
            "raw_size_mb": 0,
            "processed_size_mb": 0,
            "cached_size_mb": 0,
            "last_modified": None
        }
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            raw_path = f"{self.raw_path}/{dataset_name}"
            if os.path.exists(raw_path):
                status["raw_exists"] = True
                status["raw_size_mb"] = self._get_directory_size_mb(raw_path)
                status["last_modified"] = time.ctime(os.path.getmtime(raw_path))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            processed_path = f"{self.processed_path}/{dataset_name}"
            if os.path.exists(processed_path):
                status["processed_exists"] = True
                status["processed_size_mb"] = self._get_directory_size_mb(processed_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cached_path = f"{self.cached_path}/{dataset_name}"
            if os.path.exists(cached_path):
                status["cached_exists"] = True
                status["cached_size_mb"] = self._get_directory_size_mb(cached_path)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {e}")
        
        return status
    
    def _get_directory_size_mb(self, directory: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –ú–ë"""
        try:
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(directory):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    try:
                        total_size += os.path.getsize(filepath)
                    except (OSError, IOError):
                        continue
            return total_size / (1024 * 1024)
        except:
            return 0.0
    
    def cleanup_temp_files(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            if os.path.exists(self.temp_path):
                shutil.rmtree(self.temp_path)
                os.makedirs(self.temp_path, exist_ok=True)
                print("üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –æ—á–∏—â–µ–Ω—ã")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
    
    def get_disk_usage(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –¥–∏—Å–∫–∞"""
        usage = {}
        
        try:
            # –û–±—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            total, used, free = shutil.disk_usage(self.project_path)
            usage["total_gb"] = total / (1024**3)
            usage["used_gb"] = used / (1024**3)
            usage["free_gb"] = free / (1024**3)
            usage["used_percent"] = (used / total) * 100
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ –ø–∞–ø–∫–∞–º
            usage["datasets_gb"] = self._get_directory_size_mb(self.datasets_path) / 1024
            usage["raw_gb"] = self._get_directory_size_mb(self.raw_path) / 1024
            usage["processed_gb"] = self._get_directory_size_mb(self.processed_path) / 1024
            usage["cached_gb"] = self._get_directory_size_mb(self.cached_path) / 1024
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∏—Å–∫–µ: {e}")
        
        return usage


def quick_download_dataset(dataset_name: str, skip_preprocessing: bool = False) -> bool:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    downloader = AllanDatasetDownloader()
    return downloader.download_and_preprocess(dataset_name, skip_preprocessing)


def list_downloadable_datasets():
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    downloader = AllanDatasetDownloader()
    downloader.list_available_datasets()


def get_dataset_status(dataset_name: str) -> Dict[str, Any]:
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    downloader = AllanDatasetDownloader()
    return downloader.get_dataset_status(dataset_name)


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    downloader = AllanDatasetDownloader()
    
    print("üî• Allan Dataset Downloader - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è")
    print("=" * 60)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    downloader.list_available_datasets()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞
    print(f"\nüíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞:")
    disk_usage = downloader.get_disk_usage()
    for key, value in disk_usage.items():
        if "percent" in key:
            print(f"  {key}: {value:.1f}%")
        else:
            print(f"  {key}: {value:.2f} –ì–ë")
    
    # –ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)
    # print(f"\nüöÄ –ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ 'sberquad':")
    # success = downloader.download_and_preprocess("sberquad")
    # if success:
    #     print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!")
    # else:
    #     print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
