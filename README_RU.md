Дообучение русскоязычной LLM с QLoRA и экспорт в GGUF (для Ollama)

Краткий план
- Откройте `colab_ru_qlora_gguf.ipynb` в Google Colab.
- Смонтируйте Google Drive и укажите путь к вашему датасету (`dataset.jsonl`) или имя HF датасета.
- Запустите обучение: чекпоинты и веса сохранятся на Drive (есть возобновление).
- После обучения ноутбук сольёт LoRA в полные HF веса и сконвертирует в GGUF.
- Скачайте GGUF на Mac и запустите в Ollama.

Требования
- Google Colab (бесплатный тариф подойдёт для маленькой модели и коротких пробегов).
- Google Drive (у вас 1.5 ТБ — хватает для весов и датасетов).

Базовая модель по умолчанию
Используется `Qwen/Qwen2.5-1.5B-Instruct` — небольшая мультиязычная модель, хорошо понимает русский и подходит для Colab. Совместима с экспортом в GGUF.

Формат датасета
- JSONL (одна запись на строку). Поддерживаются:
  - `messages`: список `{role: "user"|"assistant", content: "..."}`
  - `instruction` + `input` + `output`
  - `text` (будет считаться ответом на пустой запрос)
- Или укажите имя датасета на HuggingFace через переменную окружения `DATASET_NAME`.

Полезные русские датасеты (на HF)
- `IlyaGusev/saiga_qa` — русские QA/инструкции
- `IlyaGusev/ru_sharegpt_cleaned` — русские диалоги
- `ai-forever/ru_tutorial_dialogs` — обучающие диалоги (RU)

Запуск обучения
- В ноутбуке настраивается `train_config`. Для T4/L4 оставьте как есть, при OOM уменьшайте `max_seq_length` и увеличивайте `gradient_accumulation_steps`.
- Обучение возобновляется автоматически при наличии `checkpoint-xxxx` в папке чекпоинтов на Drive.

Экспорт в GGUF и запуск в Ollama
- Ноутбук создаст `*.f16.gguf` и квантизует в `Q4_K_M` — хороший компромисс для MacBook Air M1 8GB.
- На Mac создайте `Modelfile` и выполните:
  ```bash
  ollama create ru-qlora -f Modelfile
  ollama run ru-qlora
  ```

FAQ
- Можно ли продолжить обучение позже? — Да, по чекпоинтам на Drive.
- Можно использовать другую базовую модель? — Да, задайте `BASE_MODEL`, учитывайте память/скорость.
- Какой квант для M1 8GB? — `Q4_K_M` или при нехватке `Q4_K_S`.
