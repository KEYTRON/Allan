### Советы по русскому датасету
- **Формат JSONL**: одна запись в строке. Поддерживаются поля: `messages` (список ролей `user`/`assistant`), либо `instruction`/`input`/`output`, либо просто `text`.
- **Качество**: избегайте токсичности, повторы убирайте, длину ответов держите разумной (64-512 токенов).
- **Дообучение позже**: просто добавьте новые записи в JSONL и перезапустите ноутбук — чекпоинты позволят продолжить.
## Запуск на Mac с Ollama

1) Скачайте из Google Drive файл GGUF, например `qwen2.5-1_5b-instruct-finetuned.Q4_K_M.gguf`.
2) Создайте рядом файл `Modelfile`:
```text
FROM ./qwen2.5-1_5b-instruct-finetuned.Q4_K_M.gguf

# Небольшой промпт-шаблон. Можно опустить если GGUF уже содержит chat_template
TEMPLATE """
{{ if .System }}<|system|>
{{ .System }}
{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}
<|assistant|>
{{ end }}"""
PARAMETER temperature 0.6
PARAMETER top_p 0.9
PARAMETER num_ctx 2048
```
3) Соберите модель и запустите:
```bash
ollama create ru-qlora -f Modelfile
ollama run ru-qlora
```

Подсказка: если память ограничена, используйте Q4_K_S или Q5_K_M.
# Конвертация HF -> GGUF через llama.cpp и квантизация (Q4_K_M по умолчанию)
import subprocess, sys

LLAMACPP_DIR = BASE_DIR / 'llama.cpp'
if not LLAMACPP_DIR.exists():
    !git clone -q https://github.com/ggerganov/llama.cpp {LLAMACPP_DIR}

# Устанавливаем зависимости для конвертера
%pip -q install numpy==1.26.4

convert_script = LLAMACPP_DIR / 'convert-hf-to-gguf.py'
outfile_base = GGUF_DIR / 'qwen2.5-1_5b-instruct-finetuned'

!python {convert_script} --outtype f16 --outfile {outfile_base}.f16.gguf {MERGED_DIR}

# Квантизация в Q4_K_M (хороший компромисс для M1 8GB)
quantize_bin = LLAMACPP_DIR / 'quantize'
if not (LLAMACPP_DIR / 'build').exists():
    %cd {LLAMACPP_DIR}
    !mkdir -p build && cmake -S . -B build -DGGML_METAL=OFF -DGGML_OPENBLAS=ON -DCMAKE_BUILD_TYPE=Release | cat
    !cmake --build build -j 2 | cat
    %cd -

!{LLAMACPP_DIR / 'build' / 'bin' / 'quantize'} {outfile_base}.f16.gguf {outfile_base}.Q4_K_M.gguf Q4_K_M | cat
print('GGUF written to', GGUF_DIR)
# Слияние LoRA в полные HF веса для последующей конвертации в GGUF
from peft import PeftModel

# Загружаем базовую модель в fp16 (без 4-бит) для мерджа
base_merge_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map='auto',
    trust_remote_code=True,
)

peft_model = PeftModel.from_pretrained(
    base_merge_model,
    str(CHECKPOINTS_DIR / 'lora_adapters'),
)

merged_model = peft_model.merge_and_unload()

merged_model.save_pretrained(str(MERGED_DIR), safe_serialization=True)
# используем исходный токенайзер
tokenizer.save_pretrained(str(MERGED_DIR))
print('Merged full HF model saved to', MERGED_DIR)
# Запуск обучения с возобновлением, если есть чекпоинт
latest_ckpt = None
if CHECKPOINTS_DIR.exists():
    # ищем последний checkpoint-xxxx
    subdirs = [p for p in CHECKPOINTS_DIR.glob('checkpoint-*') if p.is_dir()]
    if subdirs:
        latest_ckpt = str(sorted(subdirs, key=lambda p: int(p.name.split('-')[-1]))[-1])

print('Resuming from:', latest_ckpt)
trainer.train(resume_from_checkpoint=latest_ckpt if latest_ckpt else None)

# сохраняем только LoRA адаптеры (они маленькие)
trainer.model.save_pretrained(str(CHECKPOINTS_DIR / 'lora_adapters'))
trainer.tokenizer.save_pretrained(str(CHECKPOINTS_DIR / 'lora_adapters'))
print('Saved LoRA adapters to', CHECKPOINTS_DIR / 'lora_adapters')
# Подготовка модели и токенайзера с 4-бит загрузкой (QLoRA)
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig
from trl import SFTTrainer
from transformers import TrainingArguments

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.float16,
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map='auto',
    trust_remote_code=True,
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[
        'q_proj', 'k_proj', 'v_proj', 'o_proj',
        'gate_proj', 'up_proj', 'down_proj'
    ],
    lora_dropout=0.05,
    bias='none',
    task_type='CAUSAL_LM',
)

# Функция форматирования в строку через chat_template токенайзера (если есть)
# Для Qwen2.5-Instruct доступен chat_template; fallback на простую склейку

def format_example(ex):
    messages = ex['conversations']
    try:
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
    except Exception:
        # примитивный fallback
        parts = []
        for m in messages:
            speaker = 'User' if m['role'] == 'user' else 'Assistant'
            parts.append(f"{speaker}: {m['content']}")
        text = "\n".join(parts)
    return text

# Обёртка для SFTTrainer
from functools import partial

def formatting_func(batch):
    return [format_example(ex) for ex in batch]

args = TrainingArguments(
    output_dir=str(CHECKPOINTS_DIR),
    per_device_train_batch_size=train_config['per_device_train_batch_size'],
    gradient_accumulation_steps=train_config['gradient_accumulation_steps'],
    learning_rate=train_config['learning_rate'],
    lr_scheduler_type=train_config['lr_scheduler_type'],
    warmup_ratio=train_config['warmup_ratio'],
    weight_decay=train_config['weight_decay'],
    logging_steps=train_config['logging_steps'],
    save_steps=train_config['save_steps'],
    save_total_limit=train_config['save_total_limit'],
    bf16=train_config['bf16'],
    fp16=train_config['fp16'],
    gradient_checkpointing=train_config['gradient_checkpointing'],
    optim=train_config['optim'],
    num_train_epochs=train_config['num_train_epochs'],
    max_steps=train_config['max_steps'],
    report_to=[],
)

trainer = SFTTrainer(
    model=model,
    args=args,
    peft_config=lora_config,
    train_dataset=hf_ds,
    formatting_func=formatting_func,
    max_seq_length=train_config['max_seq_length'],
    tokenizer=tokenizer,
    packing=False,
)

print('Trainer ready. Dataset size =', len(hf_ds))
# Загрузка датасета (HF или JSONL на Drive) и приведение к формату чата для SFT
from datasets import load_dataset, Dataset
import json
from typing import List, Dict

# Поддерживаем несколько популярных форматов: Alpaca, ShareGPT, простые пары instruction/output

def load_local_jsonl(path: str) -> List[Dict]:
    rows = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows

# Приводим записи к списку сообщений чата (user/assistant)
# Поддерживает ключи: "instruction"+"input"/"output", либо "messages" (формат ShareGPT-like),
# либо одно поле "text" (будет считаться ответом на пустой промпт).

def normalize_to_conversations(example: Dict) -> List[Dict[str, str]]:
    if 'messages' in example and isinstance(example['messages'], list):
        messages = []
        for m in example['messages']:
            role = m.get('role')
            content = m.get('content')
            if role in ['user', 'assistant'] and isinstance(content, str) and content.strip():
                messages.append({'role': role, 'content': content.strip()})
        # гарантируем, что начинаем с user и чередуемся
        if not messages or messages[0]['role'] != 'user':
            return []
        return messages

    instruction = example.get('instruction')
    input_text = example.get('input') or ''
    output = example.get('output') or example.get('response')
    text = example.get('text')

    if instruction is not None and output is not None:
        user_msg = instruction.strip()
        if input_text:
            user_msg = (user_msg + "\n" + input_text.strip()).strip()
        return [
            {'role': 'user', 'content': user_msg},
            {'role': 'assistant', 'content': str(output).strip()},
        ]
    elif text is not None:
        # если только text, считаем как ответ на пустой запрос
        return [
            {'role': 'user', 'content': ''},
            {'role': 'assistant', 'content': str(text).strip()},
        ]

    return []

if DATASET_NAME:
    raw = load_dataset(DATASET_NAME, split='train')
    records = [normalize_to_conversations(ex) for ex in raw]
else:
    data = load_local_jsonl(DATASET_JSONL_PATH)
    records = [normalize_to_conversations(ex) for ex in data]

# фильтруем пустые/некорректные
conversations: List[List[Dict[str, str]]] = [r for r in records if len(r) >= 2]
print('Loaded conversations:', len(conversations))

hf_ds = Dataset.from_list([{'conversations': conv} for conv in conversations])
print(hf_ds)
# Монтаж Google Drive и конфигурация путей/гиперпараметров
from pathlib import Path
import os

try:
    from google.colab import drive  # type: ignore
    IN_COLAB = True
except Exception:
    IN_COLAB = False

if IN_COLAB:
    drive.mount('/content/drive')
    BASE_DIR = Path('/content/drive/MyDrive/ru_qlora')
else:
    BASE_DIR = Path('/content/ru_qlora')  # локально (на случай отладки вне Colab)

BASE_DIR.mkdir(parents=True, exist_ok=True)

# Базовая модель: маленькая и мульти-язычная, совместима с GGUF через llama.cpp
BASE_MODEL = os.environ.get('BASE_MODEL', 'Qwen/Qwen2.5-1.5B-Instruct')

# Датасет: укажите либо имя HF, либо путь к JSONL в Google Drive
# 1) Для HF: DATASET_NAME='IlyaGusev/saiga_qa' и поля ниже игнорируются
# 2) Для файла: DATASET_NAME='' и DATASET_JSONL_PATH указывает на файл JSONL в Drive
DATASET_NAME = os.environ.get('DATASET_NAME', '')
DATASET_JSONL_PATH = os.environ.get('DATASET_JSONL_PATH', str(BASE_DIR / 'dataset.jsonl'))

# Папки для артефактов
CHECKPOINTS_DIR = BASE_DIR / 'checkpoints'
MERGED_DIR = BASE_DIR / 'merged_full_hf'
GGUF_DIR = BASE_DIR / 'gguf'
for p in [CHECKPOINTS_DIR, MERGED_DIR, GGUF_DIR]:
    p.mkdir(parents=True, exist_ok=True)

# Тренировочные гиперпараметры (адаптируйте под вашу GPU/Colab сессию)
train_config = dict(
    num_train_epochs = 1.0,            # увеличьте при необходимости
    max_steps = -1,                    # можно выставить число шагов вместо эпох
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 8,
    learning_rate = 2e-4,
    lr_scheduler_type = 'cosine',
    warmup_ratio = 0.03,
    weight_decay = 0.0,
    logging_steps = 25,
    save_steps = 200,
    save_total_limit = 3,
    bf16 = False,                      # На T4 обычно fp16
    fp16 = True,
    gradient_checkpointing = True,
    optim = 'paged_adamw_8bit',
    max_seq_length = 1024,
)

print('BASE_MODEL =', BASE_MODEL)
print('DATASET_NAME =', DATASET_NAME)
print('DATASET_JSONL_PATH =', DATASET_JSONL_PATH)
print('Artifacts at:', BASE_DIR)
# Проверка GPU и установка зависимостей
!nvidia-smi || echo "No NVIDIA GPU detected"

%pip -q install --upgrade pip
%pip -q install transformers==4.43.3 datasets==2.20.0 accelerate==0.33.0 peft==0.12.0 bitsandbytes==0.43.3 trl==0.9.6 sentencepiece==0.2.0 einops==0.8.0

# llama.cpp для GGUF конвертации поставим позже (после сохранения full HF весов)
# Русскоязычное дообучение QLoRA с сохранением на Google Drive и экспортом в GGUF

Этот ноутбук предназначен для бесплатного Google Colab. Он:
- монтирует Google Drive и сохраняет туда датасет, контрольные точки, веса и экспорт GGUF;
- загружает русский (или мультиязычный с хорошим русским) базовый LLM (по умолчанию Qwen2.5-1.5B-Instruct);
- дообучает с QLoRA на вашем русском датасете (HF или JSONL на Drive);
- поддерживает возобновление обучения с чекпоинтов;
- сливает LoRA-адаптеры в полные HF веса;
- конвертирует в GGUF (через llama.cpp) и кладёт в Drive;
- даёт инструкции для запуска в Ollama на macOS (M1).

Примерно хватает T4/L4 на бесплатном Colab; батчи и LoRA можно ужать при нехватке памяти.
