{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f064f9",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# ü§ñ AllanGPTv1 ‚Äî –û–±—É—á–µ–Ω–∏–µ GPT-2 –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤ Google Colab\n",
    "\n",
    "# üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "!pip install -q transformers datasets accelerate\n",
    "\n",
    "# üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# üìö –ò–º–ø–æ—Ä—Ç\n",
    "from transformers import TFGPT2LMHeadModel, GPT2TokenizerFast, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ –æ—Ç –°–±–µ—Ä–∞\n",
    "model_name = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ opus100\n",
    "dataset = load_dataset(\"opus100\", lang1=\"ru\", lang2=\"en\", split=\"train\")\n",
    "\n",
    "# üîß –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥\n",
    "def preprocess(example):\n",
    "    return tokenizer(example['translation']['ru'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# üìä –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_dataset = tokenized.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer)\n",
    "model.fit(train_dataset, epochs=1)\n",
    "\n",
    "# üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ Google Drive\n",
    "save_path = \"/content/drive/MyDrive/AllanGPTv1\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}