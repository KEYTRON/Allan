#!/usr/bin/env python3
"""
Allan Dataset Manager - –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å Google Drive
"""

import os
import time
import shutil
import zipfile
import tarfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass
import psutil
import subprocess

@dataclass
class DatasetInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
    name: str
    size_mb: float
    format: str  # 'zip', 'tar', 'directory', 'hf_dataset'
    path: str
    description: str = ""
    language: str = "ru"
    task_type: str = "general"

class AllanDatasetManager:
    """–£–º–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Allan"""
    
    def __init__(self, project_path: str = "/content/drive/MyDrive/ML_Projects/Allan_Model"):
        self.project_path = project_path
        self.datasets_path = f"{project_path}/datasets"
        self.raw_path = f"{self.datasets_path}/raw"
        self.processed_path = f"{self.datasets_path}/processed"
        self.cached_path = f"{self.datasets_path}/cached"
        self.local_cache = "/content/allan_cache"
        
        # –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∑–∞–≥—Ä—É–∑–∫–∏ (–≤ –ú–ë)
        self.SMALL_DATASET_THRESHOLD = 100   # < 100 –ú–ë - —á–∏—Ç–∞—Ç—å –Ω–∞–ø—Ä—è–º—É—é —Å Drive
        self.MEDIUM_DATASET_THRESHOLD = 500  # 100-500 –ú–ë - –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ
        self.LARGE_DATASET_THRESHOLD = 2000  # > 2 –ì–ë - –ø–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö –¥–ª—è Allan
        self.recommended_datasets = {
            "sberquad": DatasetInfo(
                name="sberquad",
                size_mb=150,
                format="hf_dataset", 
                path="sberbank-ai/sberquad",
                description="–†—É—Å—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ SQuAD",
                task_type="qa"
            ),
            "rucola": DatasetInfo(
                name="rucola",
                size_mb=50,
                format="hf_dataset",
                path="RussianNLP/rucola", 
                description="–ö–æ—Ä–ø—É—Å –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–∏–µ–º–ª–µ–º–æ—Å—Ç–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ",
                task_type="classification"
            ),
            "russian_superglue": DatasetInfo(
                name="russian_superglue",
                size_mb=200,
                format="hf_dataset",
                path="russian-nlp/russian-superglue",
                description="–ù–∞–±–æ—Ä –∑–∞–¥–∞—á –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π",
                task_type="multi_task"
            ),
            "lenta_news": DatasetInfo(
                name="lenta_news", 
                size_mb=2000,
                format="hf_dataset",
                path="IlyaGusev/gazeta",
                description="–ù–æ–≤–æ—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ Lenta.ru",
                task_type="text_generation"
            ),
            "russian_poems": DatasetInfo(
                name="russian_poems",
                size_mb=150,
                format="hf_dataset", 
                path="IlyaGusev/russian_poems",
                description="–ö–æ—Ä–ø—É—Å —Ä—É—Å—Å–∫–æ–π –ø–æ—ç–∑–∏–∏",
                task_type="text_generation"
            )
        }
    
    def get_file_size_mb(self, file_path: str) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –ú–ë"""
        try:
            if os.path.isfile(file_path):
                size_bytes = os.path.getsize(file_path)
                return size_bytes / (1024 * 1024)
            elif os.path.isdir(file_path):
                total_size = 0
                for dirpath, dirnames, filenames in os.walk(file_path):
                    for filename in filenames:
                        filepath = os.path.join(dirpath, filename)
                        try:
                            total_size += os.path.getsize(filepath)
                        except (OSError, IOError):
                            continue
                return total_size / (1024 * 1024)
            else:
                return 0.0
        except Exception:
            return 0.0
    
    def get_available_space_gb(self, path: str = "/content") -> float:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ–µ –º–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ –≤ –ì–ë"""
        try:
            statvfs = os.statvfs(path)
            free_bytes = statvfs.f_frsize * statvfs.f_bavail
            return free_bytes / (1024**3)
        except:
            return 0.0
    
    def choose_loading_strategy(self, dataset_size_mb: float) -> str:
        """–í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏"""
        available_space_gb = self.get_available_space_gb()
        dataset_size_gb = dataset_size_mb / 1024
        
        print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_size_mb:.1f} –ú–ë ({dataset_size_gb:.2f} –ì–ë)")
        print(f"üíæ –î–æ—Å—Ç—É–ø–Ω–æ –º–µ—Å—Ç–∞: {available_space_gb:.1f} –ì–ë")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ –º–µ—Å—Ç–∞
        if dataset_size_gb > available_space_gb * 0.8:  # –û—Å—Ç–∞–≤–ª—è–µ–º 20% –∑–∞–ø–∞—Å–∞
            print("‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Å—Ç–∞ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è")
            return "streaming"
        
        # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É
        if dataset_size_mb < self.SMALL_DATASET_THRESHOLD:
            strategy = "direct"
            print(f"üîÑ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü—Ä—è–º–æ–µ —á—Ç–µ–Ω–∏–µ —Å Drive (< {self.SMALL_DATASET_THRESHOLD} –ú–ë)")
        elif dataset_size_mb < self.MEDIUM_DATASET_THRESHOLD:
            strategy = "copy_local"
            print(f"üîÑ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à (< {self.MEDIUM_DATASET_THRESHOLD} –ú–ë)")
        elif dataset_size_mb < self.LARGE_DATASET_THRESHOLD:
            strategy = "copy_local"
            print(f"üîÑ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à (< {self.LARGE_DATASET_THRESHOLD} –ú–ë)")
        else:
            strategy = "streaming"
            print(f"üîÑ –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (> {self.LARGE_DATASET_THRESHOLD} –ú–ë)")
        
        return strategy
    
    def extract_archive(self, archive_path: str, extract_to: str) -> bool:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞"""
        try:
            print(f"üì¶ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞: {os.path.basename(archive_path)}")
            
            if archive_path.endswith('.zip'):
                with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_to)
            elif archive_path.endswith(('.tar', '.tar.gz', '.tgz')):
                with tarfile.open(archive_path, 'r:*') as tar_ref:
                    tar_ref.extractall(extract_to)
            else:
                print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∞—Ä—Ö–∏–≤–∞: {archive_path}")
                return False
            
            print("‚úÖ –ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—Ä—Ö–∏–≤–∞: {e}")
            return False
    
    def copy_with_progress(self, src: str, dst: str) -> bool:
        """–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            file_size = os.path.getsize(src)
            print(f"üìÅ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ: {os.path.basename(src)} ({file_size / (1024*1024):.1f} –ú–ë)")
            
            # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if file_size > 50 * 1024 * 1024:  # > 50 –ú–ë
                with open(src, 'rb') as fsrc:
                    with open(dst, 'wb') as fdst:
                        copied = 0
                        chunk_size = 1024 * 1024  # 1 –ú–ë —á–∞–Ω–∫–∏
                        
                        while True:
                            chunk = fsrc.read(chunk_size)
                            if not chunk:
                                break
                            fdst.write(chunk)
                            copied += len(chunk)
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10%
                            progress = (copied / file_size) * 100
                            if copied % (file_size // 10 + 1) == 0:
                                print(f"  üìà {progress:.0f}%")
            else:
                shutil.copy2(src, dst)
            
            print("‚úÖ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return False
    
    def load_dataset_direct(self, dataset_path: str, **kwargs):
        """–ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å Google Drive"""
        print("üîó –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å Google Drive...")
        
        try:
            from datasets import load_dataset, load_from_disk
            
            if os.path.isdir(dataset_path):
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
                dataset = load_from_disk(dataset_path)
            else:
                # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ –∏–º–µ–Ω–∏/–ø—É—Ç–∏
                dataset = load_dataset(dataset_path, **kwargs)
            
            print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞–ø—Ä—è–º—É—é —Å Drive")
            return dataset
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä—è–º–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return None
    
    def load_dataset_local_copy(self, dataset_path: str, dataset_name: str, **kwargs):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        print("üíæ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ...")
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞
            local_dataset_path = f"{self.local_cache}/{dataset_name}"
            os.makedirs(local_dataset_path, exist_ok=True)
            
            # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            if os.path.isfile(dataset_path):
                # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
                local_file = f"{local_dataset_path}/{os.path.basename(dataset_path)}"
                if not self.copy_with_progress(dataset_path, local_file):
                    return None
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –µ—Å–ª–∏ —ç—Ç–æ –∞—Ä—Ö–∏–≤
                if dataset_path.endswith(('.zip', '.tar', '.tar.gz', '.tgz')):
                    if not self.extract_archive(local_file, local_dataset_path):
                        return None
                    dataset_path = local_dataset_path
                else:
                    dataset_path = local_file
                    
            elif os.path.isdir(dataset_path):
                # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                print(f"üìÅ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏...")
                shutil.copytree(dataset_path, local_dataset_path, dirs_exist_ok=True)
                dataset_path = local_dataset_path
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞
            from datasets import load_dataset, load_from_disk
            
            if os.path.isdir(dataset_path) and any(f.endswith('.arrow') for f in os.listdir(dataset_path)):
                dataset = load_from_disk(dataset_path)
            else:
                dataset = load_dataset(dataset_path, **kwargs)
            
            print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞")
            return dataset
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return None
    
    def load_dataset_streaming(self, dataset_path: str, **kwargs):
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print("üåä –ü–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        try:
            from datasets import load_dataset
            
            # –í–∫–ª—é—á–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤—ã–π —Ä–µ–∂–∏–º
            kwargs['streaming'] = True
            dataset = load_dataset(dataset_path, **kwargs)
            
            print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º —Ä–µ–∂–∏–º–µ")
            return dataset
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return None
    
    def load_huggingface_dataset(self, dataset_name: str, **kwargs):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Hugging Face Hub"""
        print(f"ü§ó –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ Hugging Face: {dataset_name}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
        if dataset_name in self.recommended_datasets:
            dataset_info = self.recommended_datasets[dataset_name]
            dataset_path = dataset_info.path
            size_mb = dataset_info.size_mb
            
            print(f"üìù {dataset_info.description}")
            print(f"üéØ –¢–∏–ø –∑–∞–¥–∞—á–∏: {dataset_info.task_type}")
        else:
            dataset_path = dataset_name
            size_mb = 100  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∑–∞–≥—Ä—É–∑–∫–∏
        strategy = self.choose_loading_strategy(size_mb)
        
        try:
            from datasets import load_dataset
            
            if strategy == "streaming":
                kwargs['streaming'] = True
                dataset = load_dataset(dataset_path, **kwargs)
            else:
                dataset = load_dataset(dataset_path, **kwargs)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –Ω–∞ Drive –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                if strategy == "copy_local":
                    cache_path = f"{self.cached_path}/{dataset_name}"
                    os.makedirs(cache_path, exist_ok=True)
                    dataset.save_to_disk(cache_path)
                    print(f"üíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à: {cache_path}")
            
            print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
            return dataset
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return None
    
    def load_dataset(self, dataset_source: str, **kwargs):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_source}")
        print("=" * 50)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        if dataset_source in self.recommended_datasets:
            # –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            return self.load_huggingface_dataset(dataset_source, **kwargs)
        
        elif os.path.exists(dataset_source):
            # –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª/–ø–∞–ø–∫–∞
            size_mb = self.get_file_size_mb(dataset_source)
            strategy = self.choose_loading_strategy(size_mb)
            
            if strategy == "direct":
                return self.load_dataset_direct(dataset_source, **kwargs)
            elif strategy == "copy_local":
                dataset_name = os.path.basename(dataset_source)
                return self.load_dataset_local_copy(dataset_source, dataset_name, **kwargs)
            else:  # streaming
                return self.load_dataset_streaming(dataset_source, **kwargs)
        
        else:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —ç—Ç–æ –ø—É—Ç—å Hugging Face
            return self.load_huggingface_dataset(dataset_source, **kwargs)
    
    def list_available_datasets(self):
        """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
        print("üìö –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è Allan:")
        print("=" * 60)
        
        for name, info in self.recommended_datasets.items():
            print(f"\nüîπ {name}")
            print(f"  üìä –†–∞–∑–º–µ—Ä: {info.size_mb} –ú–ë")
            print(f"  üéØ –ó–∞–¥–∞—á–∞: {info.task_type}")
            print(f"  üìù {info.description}")
            print(f"  üîó –ü—É—Ç—å: {info.path}")
    
    def get_dataset_stats(self, dataset, dataset_name: str = "dataset"):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ '{dataset_name}':")
        
        try:
            if hasattr(dataset, 'num_rows'):
                print(f"  üìà –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {dataset.num_rows:,}")
            
            if hasattr(dataset, 'features'):
                print(f"  üè∑Ô∏è  –ü—Ä–∏–∑–Ω–∞–∫–∏: {list(dataset.features.keys())}")
            
            if hasattr(dataset, 'column_names'):
                print(f"  üìã –°—Ç–æ–ª–±—Ü—ã: {dataset.column_names}")
            
            # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
            if hasattr(dataset, '__iter__'):
                print("  üìù –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
                try:
                    example = next(iter(dataset))
                    for key, value in list(example.items())[:3]:  # –ü–µ—Ä–≤—ã–µ 3 –ø–æ–ª—è
                        value_str = str(value)[:100] + "..." if len(str(value)) > 100 else str(value)
                        print(f"    {key}: {value_str}")
                except:
                    pass
                    
        except Exception as e:
            print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
    
    def clear_local_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞"""
        print("üßπ –û—á–∏—Å—Ç–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞...")
        
        try:
            if os.path.exists(self.local_cache):
                shutil.rmtree(self.local_cache)
                print("‚úÖ –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –æ—á–∏—â–µ–Ω")
            
            os.makedirs(self.local_cache, exist_ok=True)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞: {e}")
    
    def monitor_resources(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤"""
        print("üîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤:")
        
        try:
            # RAM
            memory = psutil.virtual_memory()
            print(f"  üß† RAM: {memory.percent:.1f}% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ ({memory.used / (1024**3):.1f}/{memory.total / (1024**3):.1f} –ì–ë)")
            
            # –î–∏—Å–∫
            disk = psutil.disk_usage("/content")
            disk_percent = (disk.used / disk.total) * 100
            print(f"  üíæ –î–∏—Å–∫: {disk_percent:.1f}% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ ({disk.used / (1024**3):.1f}/{disk.total / (1024**3):.1f} –ì–ë)")
            
            # GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory
                    gpu_allocated = torch.cuda.memory_allocated(0)
                    gpu_percent = (gpu_allocated / gpu_memory) * 100
                    print(f"  üéÆ GPU: {gpu_percent:.1f}% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ ({gpu_allocated / (1024**3):.1f}/{gpu_memory / (1024**3):.1f} –ì–ë)")
            except:
                pass
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {e}")


def quick_load_dataset(dataset_name: str, **kwargs):
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    manager = AllanDatasetManager()
    return manager.load_dataset(dataset_name, **kwargs)


def list_datasets():
    """–ë—ã—Å—Ç—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤"""
    manager = AllanDatasetManager()
    manager.list_available_datasets()


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    manager = AllanDatasetManager()
    
    print("üî• Allan Dataset Manager - –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è")
    print("=" * 50)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    manager.list_available_datasets()
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
    print("\n")
    manager.monitor_resources()